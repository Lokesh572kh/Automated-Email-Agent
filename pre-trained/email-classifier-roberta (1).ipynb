{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"sentiment-analysis-using-roberta.ipynb","provenance":[],"collapsed_sections":[],"include_colab_link":true},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"30864762e7f242c281b72862c5c08a33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dd12a39995584ba79f0e786b370b1a99","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b89c9e76b5594a8ea601b9c5d2af4fa6","IPY_MODEL_f65c7649640b4e87a7819a3da2f54fe0"]}},"dd12a39995584ba79f0e786b370b1a99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b89c9e76b5594a8ea601b9c5d2af4fa6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_80a6b6c9c4d5436ebe3b90b791c6fd93","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":898823,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":898823,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8c1f6e94723842faa6bd3dcd9ff4ea82"}},"f65c7649640b4e87a7819a3da2f54fe0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_39b5ca071fd3452e9d9145dd2b366da1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 899k/899k [00:02&lt;00:00, 333kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f8dfd3ea6bb7413592115195bc6e0b83"}},"80a6b6c9c4d5436ebe3b90b791c6fd93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8c1f6e94723842faa6bd3dcd9ff4ea82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"39b5ca071fd3452e9d9145dd2b366da1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f8dfd3ea6bb7413592115195bc6e0b83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"611dfdca86f4498e8aa1491ed6ffb13d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_be4857f17c244fb39a771f2c97283fd5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2fe41e1db18b4295a6907771462a0fce","IPY_MODEL_0b29a9e1a275451bbc2114807532f91e"]}},"be4857f17c244fb39a771f2c97283fd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2fe41e1db18b4295a6907771462a0fce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_115c8809853d410fac6e7f69af5a5488","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":456318,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":456318,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_390827d7d2cb4b4fbfc0c022f015f7ed"}},"0b29a9e1a275451bbc2114807532f91e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5d305d4db08f47ba91461edb343874a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 456k/456k [00:00&lt;00:00, 472kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_773af3ca0add4e7cac0a036fb8b55632"}},"115c8809853d410fac6e7f69af5a5488":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"390827d7d2cb4b4fbfc0c022f015f7ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d305d4db08f47ba91461edb343874a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"773af3ca0add4e7cac0a036fb8b55632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9593906,"sourceType":"datasetVersion","datasetId":5851957},{"sourceId":9600529,"sourceType":"datasetVersion","datasetId":5856901}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"!pip install transformers==3.0.2","metadata":{"id":"a-GlywkSFegL","trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:00:12.328775Z","iopub.execute_input":"2024-10-11T12:00:12.329242Z","iopub.status.idle":"2024-10-11T12:00:27.926827Z","shell.execute_reply.started":"2024-10-11T12:00:12.329208Z","shell.execute_reply":"2024-10-11T12:00:27.925706Z"}},"outputs":[{"name":"stdout","text":"Collecting transformers==3.0.2\n  Using cached transformers-3.0.2-py3-none-any.whl.metadata (44 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from transformers==3.0.2) (1.26.4)\nCollecting tokenizers==0.8.1.rc1 (from transformers==3.0.2)\n  Using cached tokenizers-0.8.1rc1.tar.gz (97 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from transformers==3.0.2) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==3.0.2) (3.15.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==3.0.2) (2.32.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==3.0.2) (4.66.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==3.0.2) (2024.5.15)\nRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.10/site-packages (from transformers==3.0.2) (0.2.0)\nCollecting sacremoses (from transformers==3.0.2)\n  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->transformers==3.0.2) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==3.0.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==3.0.2) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==3.0.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==3.0.2) (2024.8.30)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from sacremoses->transformers==3.0.2) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from sacremoses->transformers==3.0.2) (1.4.2)\nUsing cached transformers-3.0.2-py3-none-any.whl (769 kB)\nUsing cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\nBuilding wheels for collected packages: tokenizers\n  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25lerror\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m \u001b[31m[46 lines of output]\u001b[0m\n  \u001b[31m   \u001b[0m /tmp/pip-build-env-fnx7ll_3/overlay/lib/python3.10/site-packages/setuptools/dist.py:294: InformationOnly: Normalizing '0.8.1.rc1' to '0.8.1rc1'\n  \u001b[31m   \u001b[0m   self.metadata.version = self._normalize_version(self.metadata.version)\n  \u001b[31m   \u001b[0m running bdist_wheel\n  \u001b[31m   \u001b[0m running build\n  \u001b[31m   \u001b[0m running build_py\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers\n  \u001b[31m   \u001b[0m copying tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/models\n  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/processors\n  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying tokenizers/implementations/__init__.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying tokenizers/implementations/base_tokenizer.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying tokenizers/implementations/byte_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying tokenizers/implementations/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying tokenizers/implementations/char_level_bpe.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying tokenizers/implementations/bert_wordpiece.py -> build/lib.linux-x86_64-cpython-310/tokenizers/implementations\n  \u001b[31m   \u001b[0m copying tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers\n  \u001b[31m   \u001b[0m copying tokenizers/models/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/models\n  \u001b[31m   \u001b[0m copying tokenizers/decoders/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/decoders\n  \u001b[31m   \u001b[0m copying tokenizers/normalizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/normalizers\n  \u001b[31m   \u001b[0m copying tokenizers/pre_tokenizers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/pre_tokenizers\n  \u001b[31m   \u001b[0m copying tokenizers/processors/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/processors\n  \u001b[31m   \u001b[0m copying tokenizers/trainers/__init__.pyi -> build/lib.linux-x86_64-cpython-310/tokenizers/trainers\n  \u001b[31m   \u001b[0m running build_ext\n  \u001b[31m   \u001b[0m running build_rust\n  \u001b[31m   \u001b[0m error: can't find Rust compiler\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m To update pip, run:\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m     pip install --upgrade pip\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m and then retry package installation.\n  \u001b[31m   \u001b[0m \n  \u001b[31m   \u001b[0m If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n\u001b[0m\u001b[?25hFailed to build tokenizers\n\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\n\nimport pandas as pd\n\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\n\nimport seaborn as sns\n\nimport transformers\n\nimport json\n\nfrom tqdm import tqdm\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import RobertaModel, RobertaTokenizer\n\nimport logging\n\nlogging.basicConfig(level=logging.ERROR)","metadata":{"trusted":true,"_uuid":"e7b5f5ab6f8f300c8900321a91b9340376c986f2","id":"979OUro5Eac3","execution":{"iopub.status.busy":"2024-10-11T12:00:04.351477Z","iopub.execute_input":"2024-10-11T12:00:04.351800Z","iopub.status.idle":"2024-10-11T12:00:12.327534Z","shell.execute_reply.started":"2024-10-11T12:00:04.351762Z","shell.execute_reply":"2024-10-11T12:00:12.326551Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\nfrom torch import cuda\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"id":"sb1Q5N6LGK7z","trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:00:27.929107Z","iopub.execute_input":"2024-10-11T12:00:27.929455Z","iopub.status.idle":"2024-10-11T12:00:27.987613Z","shell.execute_reply.started":"2024-10-11T12:00:27.929415Z","shell.execute_reply":"2024-10-11T12:00:27.986503Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/new-data/new_dataset.csv')","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"id":"J3FzcAlgEac8","execution":{"iopub.status.busy":"2024-10-11T12:00:27.989164Z","iopub.execute_input":"2024-10-11T12:00:27.989507Z","iopub.status.idle":"2024-10-11T12:00:28.012277Z","shell.execute_reply.started":"2024-10-11T12:00:27.989472Z","shell.execute_reply":"2024-10-11T12:00:28.011542Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train[\"Class\"] = train[\"Class\"].replace({\n    'student': 0,\n    'researcher': 1,\n    'corporates': 2\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:03:38.415560Z","iopub.execute_input":"2024-10-11T12:03:38.415993Z","iopub.status.idle":"2024-10-11T12:03:38.428142Z","shell.execute_reply.started":"2024-10-11T12:03:38.415954Z","shell.execute_reply":"2024-10-11T12:03:38.426943Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3921634441.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  train[\"Class\"] = train[\"Class\"].replace({\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train.shape","metadata":{"id":"TFIoIjucGjJw","outputId":"52fda1fa-cb54-4eb2-c142-c90b5f958edf","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:03:39.608407Z","iopub.execute_input":"2024-10-11T12:03:39.609419Z","iopub.status.idle":"2024-10-11T12:03:39.616576Z","shell.execute_reply.started":"2024-10-11T12:03:39.609368Z","shell.execute_reply":"2024-10-11T12:03:39.615678Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(441, 2)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"_uuid":"c8dee062192ea016c0d306d3441ae2c573e2183c","id":"aTsOsl4MEadB","outputId":"ada765d8-2547-4196-9d73-c33c2b3bda39","colab":{"base_uri":"https://localhost:8080/","height":204},"execution":{"iopub.status.busy":"2024-10-11T12:03:41.447167Z","iopub.execute_input":"2024-10-11T12:03:41.448060Z","iopub.status.idle":"2024-10-11T12:03:41.462138Z","shell.execute_reply.started":"2024-10-11T12:03:41.448018Z","shell.execute_reply":"2024-10-11T12:03:41.461329Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                               Email  Class\n0  Dear Dr. Smith, I am reaching out to inquire a...      1\n1  I am writing to request access to the universi...      1\n2  I am a Ph.D.student in need of guidance on my ...      1\n3  I am interested in collaborating with your res...      1\n4  I am having trouble with my research methodolo...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Email</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Dear Dr. Smith, I am reaching out to inquire a...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I am writing to request access to the universi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I am a Ph.D.student in need of guidance on my ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I am interested in collaborating with your res...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I am having trouble with my research methodolo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"train[\"Class\"].unique()","metadata":{"id":"lGcvxwWXIbfq","outputId":"e5fe3e91-acba-4249-9ce9-d18fb30bc575","colab":{"base_uri":"https://localhost:8080/","height":34},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:03:45.158035Z","iopub.execute_input":"2024-10-11T12:03:45.158859Z","iopub.status.idle":"2024-10-11T12:03:45.167903Z","shell.execute_reply.started":"2024-10-11T12:03:45.158812Z","shell.execute_reply":"2024-10-11T12:03:45.166880Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([1, 0, 2])"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"new_df = train[['Email', 'Class']]","metadata":{"trusted":true,"_uuid":"01da38cc4626a85b73fbb526d9a8d128d1fd9338","id":"baSmeDdIEadM","execution":{"iopub.status.busy":"2024-10-11T12:03:48.771775Z","iopub.execute_input":"2024-10-11T12:03:48.772415Z","iopub.status.idle":"2024-10-11T12:03:48.782570Z","shell.execute_reply.started":"2024-10-11T12:03:48.772360Z","shell.execute_reply":"2024-10-11T12:03:48.781763Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"\nMAX_LEN = 256\n\nTRAIN_BATCH_SIZE = 2\n\nVALID_BATCH_SIZE = 2\n\nLEARNING_RATE = 1e-05\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)","metadata":{"id":"nvXxpfNCGER2","outputId":"d7281fe1-0dbf-42d7-c1e0-b51c4231c9c0","colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["30864762e7f242c281b72862c5c08a33","dd12a39995584ba79f0e786b370b1a99","b89c9e76b5594a8ea601b9c5d2af4fa6","f65c7649640b4e87a7819a3da2f54fe0","80a6b6c9c4d5436ebe3b90b791c6fd93","8c1f6e94723842faa6bd3dcd9ff4ea82","39b5ca071fd3452e9d9145dd2b366da1","f8dfd3ea6bb7413592115195bc6e0b83","611dfdca86f4498e8aa1491ed6ffb13d","be4857f17c244fb39a771f2c97283fd5","2fe41e1db18b4295a6907771462a0fce","0b29a9e1a275451bbc2114807532f91e","115c8809853d410fac6e7f69af5a5488","390827d7d2cb4b4fbfc0c022f015f7ed","5d305d4db08f47ba91461edb343874a4","773af3ca0add4e7cac0a036fb8b55632"]},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:03.664140Z","iopub.execute_input":"2024-10-11T12:04:03.664856Z","iopub.status.idle":"2024-10-11T12:04:04.664863Z","shell.execute_reply.started":"2024-10-11T12:04:03.664811Z","shell.execute_reply":"2024-10-11T12:04:04.663867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4c8d4fe4b1a46429e87cc41f83b0f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04f3797f18954d04b13b75c17cae7f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53df903d2cd4df19dca9aeddf6eb466"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c8de30278734d3aa94e7e2ee7e18b56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82334b8eccd043f3b602b289274ae568"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"class EmailData(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n\n        self.tokenizer = tokenizer\n\n        self.data = dataframe\n\n        self.text = dataframe.Email\n\n        self.targets = self.data.Class\n\n        self.max_len = max_len\n\n\n\n    def __len__(self):\n\n        return len(self.text)\n\n\n\n    def __getitem__(self, index):\n\n        text = str(self.text[index])\n\n        text = \" \".join(text.split())\n\n\n\n        inputs = self.tokenizer.encode_plus(\n\n            text,\n\n            None,\n\n            add_special_tokens=True,\n\n            max_length=self.max_len,\n\n            pad_to_max_length=True,\n\n            return_token_type_ids=True\n\n        )\n\n        ids = inputs['input_ids']\n\n        mask = inputs['attention_mask']\n\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n\n\n\n        return {\n\n            'ids': torch.tensor(ids, dtype=torch.long),\n\n            'mask': torch.tensor(mask, dtype=torch.long),\n\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n\n        }","metadata":{"id":"3vWRDemOGxJD","trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:08.686719Z","iopub.execute_input":"2024-10-11T12:04:08.687425Z","iopub.status.idle":"2024-10-11T12:04:08.697137Z","shell.execute_reply.started":"2024-10-11T12:04:08.687370Z","shell.execute_reply":"2024-10-11T12:04:08.696088Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"train_size = 0.7\n\ntrain_data=new_df.sample(frac=train_size,random_state=200)\n\ntest_data=new_df.drop(train_data.index).reset_index(drop=True)\n\ntrain_data = train_data.reset_index(drop=True)\n\n\n\n\n\nprint(\"FULL Dataset: {}\".format(new_df.shape))\n\nprint(\"TRAIN Dataset: {}\".format(train_data.shape))\n\nprint(\"TEST Dataset: {}\".format(test_data.shape))\n\n\n\ntraining_set = EmailData(train_data, tokenizer, MAX_LEN)\n\ntesting_set = EmailData(test_data, tokenizer, MAX_LEN)","metadata":{"id":"7Gpe9D1QHoCd","outputId":"7fc7fc2e-68a2-44b7-8e80-6bb6ce6c178b","colab":{"base_uri":"https://localhost:8080/","height":68},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:10.593213Z","iopub.execute_input":"2024-10-11T12:04:10.593870Z","iopub.status.idle":"2024-10-11T12:04:10.607757Z","shell.execute_reply.started":"2024-10-11T12:04:10.593828Z","shell.execute_reply":"2024-10-11T12:04:10.606634Z"}},"outputs":[{"name":"stdout","text":"FULL Dataset: (441, 2)\nTRAIN Dataset: (309, 2)\nTEST Dataset: (132, 2)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n\n                'shuffle': True,\n\n                'num_workers': 0\n\n                }\n\n\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n\n                'shuffle': True,\n\n                'num_workers': 0\n\n                }\n\n\n\ntraining_loader = DataLoader(training_set, **train_params)\n\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"trusted":true,"_uuid":"9fc198d13d7f33dc70588c3f22bc7b7c4f4ebb45","id":"c1tInLk2Eadt","execution":{"iopub.status.busy":"2024-10-11T12:04:12.214662Z","iopub.execute_input":"2024-10-11T12:04:12.215538Z","iopub.status.idle":"2024-10-11T12:04:12.221422Z","shell.execute_reply.started":"2024-10-11T12:04:12.215495Z","shell.execute_reply":"2024-10-11T12:04:12.220227Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class RobertaClass(torch.nn.Module):\n\n    def __init__(self):\n\n        super(RobertaClass, self).__init__()\n\n        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n\n        self.pre_classifier = torch.nn.Linear(768, 768)\n\n        self.dropout = torch.nn.Dropout(0.3)\n\n        self.classifier = torch.nn.Linear(768, 3)\n\n\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\n        hidden_state = output_1[0]\n\n        pooler = hidden_state[:, 0]\n\n        pooler = self.pre_classifier(pooler)\n\n        pooler = torch.nn.ReLU()(pooler)\n\n        pooler = self.dropout(pooler)\n\n        output = self.classifier(pooler)\n\n        return output","metadata":{"trusted":true,"_uuid":"cb8f194ee79d76356be0002b0e18f947e1412d66","id":"HMqQTafXEaei","execution":{"iopub.status.busy":"2024-10-11T12:04:17.740535Z","iopub.execute_input":"2024-10-11T12:04:17.740915Z","iopub.status.idle":"2024-10-11T12:04:17.748701Z","shell.execute_reply.started":"2024-10-11T12:04:17.740881Z","shell.execute_reply":"2024-10-11T12:04:17.747847Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"model = RobertaClass()\n\nmodel.to(device)","metadata":{"id":"sZ55mIPZIkp_","outputId":"35048672-7bf0-44bc-8fd5-b9ffbd207a17","colab":{"base_uri":"https://localhost:8080/","height":1000},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:19.849098Z","iopub.execute_input":"2024-10-11T12:04:19.849495Z","iopub.status.idle":"2024-10-11T12:04:22.767776Z","shell.execute_reply.started":"2024-10-11T12:04:19.849455Z","shell.execute_reply":"2024-10-11T12:04:22.766833Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5256c0cf6ee54b74951c8c7571124dc0"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"RobertaClass(\n  (l1): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"loss_function = torch.nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"id":"XYZ7YuJ5InOS","trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:30.878614Z","iopub.execute_input":"2024-10-11T12:04:30.879010Z","iopub.status.idle":"2024-10-11T12:04:31.366230Z","shell.execute_reply.started":"2024-10-11T12:04:30.878970Z","shell.execute_reply":"2024-10-11T12:04:31.365433Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def calcuate_accuracy(preds, targets):\n\n    n_correct = (preds==targets).sum().item()\n\n    return n_correct","metadata":{"id":"yPhA2V3iIpzN","trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:31.617778Z","iopub.execute_input":"2024-10-11T12:04:31.618286Z","iopub.status.idle":"2024-10-11T12:04:31.622973Z","shell.execute_reply.started":"2024-10-11T12:04:31.618249Z","shell.execute_reply":"2024-10-11T12:04:31.621945Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\n\ndef train(epoch):\n\n    tr_loss = 0\n\n    n_correct = 0\n\n    nb_tr_steps = 0\n\n    nb_tr_examples = 0\n\n    model.train()\n\n    for _,data in tqdm(enumerate(training_loader, 0)):\n\n        ids = data['ids'].to(device, dtype = torch.long)\n\n        mask = data['mask'].to(device, dtype = torch.long)\n\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n\n        targets = data['targets'].to(device, dtype = torch.long)\n\n\n\n        outputs = model(ids, mask, token_type_ids)\n\n        loss = loss_function(outputs, targets)\n\n        tr_loss += loss.item()\n\n        big_val, big_idx = torch.max(outputs.data, dim=1)\n\n        n_correct += calcuate_accuracy(big_idx, targets)\n\n\n\n        nb_tr_steps += 1\n\n        nb_tr_examples+=targets.size(0)\n\n        \n\n        if _%5000==0:\n\n            loss_step = tr_loss/nb_tr_steps\n\n            accu_step = (n_correct*100)/nb_tr_examples \n\n            print(f\"Training Loss per 5000 steps: {loss_step}\")\n\n            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n\n\n\n        optimizer.zero_grad()\n\n        loss.backward()\n\n        # # When using GPU\n\n        optimizer.step()\n\n\n\n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n\n    epoch_loss = tr_loss/nb_tr_steps\n\n    epoch_accu = (n_correct*100)/nb_tr_examples\n\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n\n\n    return ","metadata":{"id":"mhqvtY2SIup7","trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:36.987362Z","iopub.execute_input":"2024-10-11T12:04:36.987779Z","iopub.status.idle":"2024-10-11T12:04:36.998526Z","shell.execute_reply.started":"2024-10-11T12:04:36.987741Z","shell.execute_reply":"2024-10-11T12:04:36.997068Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"EPOCHS = 5\n\nfor epoch in range(EPOCHS):\n\n    train(epoch)","metadata":{"id":"Afn7xaunJHnI","outputId":"4ca0a58f-3d9f-432f-9da4-0c74f3b1cb02","colab":{"base_uri":"https://localhost:8080/","height":221},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:04:40.751130Z","iopub.execute_input":"2024-10-11T12:04:40.752127Z","iopub.status.idle":"2024-10-11T12:06:12.914189Z","shell.execute_reply.started":"2024-10-11T12:04:40.752084Z","shell.execute_reply":"2024-10-11T12:06:12.913155Z"}},"outputs":[{"name":"stderr","text":"0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2837: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 1.130231261253357\nTraining Accuracy per 5000 steps: 0.0\n","output_type":"stream"},{"name":"stderr","text":"155it [00:18,  8.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 0: 70.87378640776699\nTraining Loss Epoch: 0.6521657913442581\nTraining Accuracy Epoch: 70.87378640776699\n","output_type":"stream"},{"name":"stderr","text":"2it [00:00, 10.60it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.14810603857040405\nTraining Accuracy per 5000 steps: 100.0\n","output_type":"stream"},{"name":"stderr","text":"155it [00:17,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 1: 99.67637540453075\nTraining Loss Epoch: 0.05023316064729325\nTraining Accuracy Epoch: 99.67637540453075\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.009775763377547264\nTraining Accuracy per 5000 steps: 100.0\n","output_type":"stream"},{"name":"stderr","text":"155it [00:18,  8.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 2: 100.0\nTraining Loss Epoch: 0.012720925072508474\nTraining Accuracy Epoch: 100.0\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.010791252367198467\nTraining Accuracy per 5000 steps: 100.0\n","output_type":"stream"},{"name":"stderr","text":"155it [00:18,  8.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 3: 100.0\nTraining Loss Epoch: 0.005937779787927866\nTraining Accuracy Epoch: 100.0\n","output_type":"stream"},{"name":"stderr","text":"0it [00:00, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Training Loss per 5000 steps: 0.004064043518155813\nTraining Accuracy per 5000 steps: 100.0\n","output_type":"stream"},{"name":"stderr","text":"155it [00:19,  8.06it/s]","output_type":"stream"},{"name":"stdout","text":"The Total Accuracy for Epoch 4: 100.0\nTraining Loss Epoch: 0.003714523272168252\nTraining Accuracy Epoch: 100.0\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"def valid(model, testing_loader):\n\n    model.eval()\n\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n\n    with torch.no_grad():\n\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n\n            ids = data['ids'].to(device, dtype = torch.long)\n\n            mask = data['mask'].to(device, dtype = torch.long)\n\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n\n            targets = data['targets'].to(device, dtype = torch.long)\n\n            outputs = model(ids, mask, token_type_ids).squeeze()\n\n            loss = loss_function(outputs, targets)\n\n            tr_loss += loss.item()\n\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n\n            n_correct += calcuate_accuracy(big_idx, targets)\n\n\n\n            nb_tr_steps += 1\n\n            nb_tr_examples+=targets.size(0)\n\n            \n\n            if _%5000==0:\n\n                loss_step = tr_loss/nb_tr_steps\n\n                accu_step = (n_correct*100)/nb_tr_examples\n\n                print(f\"Validation Loss per 100 steps: {loss_step}\")\n\n                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n\n    epoch_loss = tr_loss/nb_tr_steps\n\n    epoch_accu = (n_correct*100)/nb_tr_examples\n\n    print(f\"Validation Loss Epoch: {epoch_loss}\")\n\n    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n\n    \n\n    return epoch_accu\n","metadata":{"id":"bFiNcy16JLwt","trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:06:23.569902Z","iopub.execute_input":"2024-10-11T12:06:23.570331Z","iopub.status.idle":"2024-10-11T12:06:23.580526Z","shell.execute_reply.started":"2024-10-11T12:06:23.570290Z","shell.execute_reply":"2024-10-11T12:06:23.579521Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"acc = valid(model, testing_loader)\n\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","metadata":{"id":"UcUylInzKdV-","outputId":"ebb887fc-04a6-4a44-cb50-9fb8b17853ea","colab":{"base_uri":"https://localhost:8080/","height":153},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:06:27.593558Z","iopub.execute_input":"2024-10-11T12:06:27.594215Z","iopub.status.idle":"2024-10-11T12:06:29.608061Z","shell.execute_reply.started":"2024-10-11T12:06:27.594177Z","shell.execute_reply":"2024-10-11T12:06:29.606972Z"}},"outputs":[{"name":"stderr","text":"3it [00:00, 21.99it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss per 100 steps: 0.0008099359693005681\nValidation Accuracy per 100 steps: 100.0\n","output_type":"stream"},{"name":"stderr","text":"66it [00:02, 32.94it/s]","output_type":"stream"},{"name":"stdout","text":"Validation Loss Epoch: 0.004147799305483755\nValidation Accuracy Epoch: 100.0\nAccuracy on test data = 100.00%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"\noutput_model_file = 'pytorch_roberta_email_state_dict.pt'\noutput_vocab_file = './'\n\nmodel_to_save = model.state_dict()\ntorch.save(model_to_save, output_model_file)\n\ntokenizer.save_vocabulary(output_vocab_file)\n\nprint('All files saved')\n","metadata":{"id":"8eKt004BKjyT","outputId":"8f43c6b5-8772-4158-f8cc-f5bbd72b2f14","colab":{"base_uri":"https://localhost:8080/","height":51},"trusted":true,"execution":{"iopub.status.busy":"2024-10-11T12:06:50.388326Z","iopub.execute_input":"2024-10-11T12:06:50.389198Z","iopub.status.idle":"2024-10-11T12:06:51.662247Z","shell.execute_reply.started":"2024-10-11T12:06:50.389157Z","shell.execute_reply":"2024-10-11T12:06:51.661256Z"}},"outputs":[{"name":"stdout","text":"All files saved\n","output_type":"stream"}],"execution_count":24}]}